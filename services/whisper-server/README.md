# ğŸ™ï¸ Lokale Whisper-Transkription

Dieser Service transkribiert Audio **lokal und schnell** mit [faster-whisper](https://github.com/SYSTRAN/faster-whisper) - ohne Cloud, ohne n8n!

## âš¡ Schnellstart

```bash
cd services/whisper-server
./start.sh
```

Das war's! Der Server lÃ¤uft jetzt auf `http://localhost:9000` ğŸš€

## ğŸ“‹ Was passiert beim Start?

Das `start.sh` Script macht automatisch:
1. âœ… Erstellt Python Virtual Environment (falls nicht vorhanden)
2. âœ… Installiert alle Dependencies
3. âœ… LÃ¤dt das Whisper-Model (beim ersten Mal ~150MB Download)
4. âœ… Startet den Server auf Port 9000

## ğŸ¯ Verwendung

### Mit dem Frontend

1. **Whisper-Server starten** (in einem Terminal):
   ```bash
   cd services/whisper-server
   ./start.sh
   ```

2. **Frontend starten** (in einem anderen Terminal):
   ```bash
   npm run dev
   ```

3. **Audio aufnehmen** â†’ Automatische Transkription â†’ BestÃ¤tigen âœ¨

### API-Endpunkte

#### POST `/transcribe-file` (empfohlen - schneller)
```bash
curl -X POST http://localhost:9000/transcribe-file \
  -F "file=@recording.wav" \
  -F "language=de"
```

Response:
```json
{
  "text": "Hallo, das ist ein Test",
  "language": "de",
  "duration": 2.5
}
```

#### POST `/transcribe` (Base64)
```bash
curl -X POST http://localhost:9000/transcribe \
  -H "Content-Type: application/json" \
  -d '{
    "audio": "<base64-string>",
    "language": "de"
  }'
```

#### GET `/healthz`
```bash
curl http://localhost:9000/healthz
```

## âš™ï¸ Konfiguration

Umgebungsvariablen in `start.sh` anpassen:

```bash
export WHISPER_MODEL=base      # tiny, base, small, medium, large-v3
export WHISPER_DEVICE=cpu      # oder "cuda" fÃ¼r GPU
export WHISPER_COMPUTE_TYPE=int8  # oder "fp16" bei GPU
```

### Model-GrÃ¶ÃŸen (Geschwindigkeit vs. Genauigkeit)

| Model | GrÃ¶ÃŸe | Geschwindigkeit | Genauigkeit | Empfohlen fÃ¼r |
|-------|-------|-----------------|-------------|---------------|
| `tiny` | ~75 MB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | Sehr schnelle Tests |
| `base` | ~150 MB | âš¡âš¡âš¡âš¡ | â­â­â­ | **Standard (empfohlen)** |
| `small` | ~500 MB | âš¡âš¡âš¡ | â­â­â­â­ | Bessere QualitÃ¤t |
| `medium` | ~1.5 GB | âš¡âš¡ | â­â­â­â­â­ | Beste QualitÃ¤t (CPU) |
| `large-v3` | ~3 GB | âš¡ | â­â­â­â­â­ | Beste QualitÃ¤t (GPU empfohlen) |

**Tipp**: `base` ist der beste Kompromiss fÃ¼r lokale CPU-Transkription! ğŸ¯

## ğŸš€ Performance-Tipps

### FÃ¼r maximale Geschwindigkeit:
- Model: `tiny` oder `base`
- `beam_size=1` (bereits gesetzt)
- `vad_filter=True` (bereits gesetzt - filtert Stille)

### FÃ¼r beste QualitÃ¤t:
- Model: `small` oder `medium`
- GPU verwenden (CUDA)

### Typische Transkriptionszeiten (CPU):

| Audio-LÃ¤nge | Model `base` | Model `small` |
|-------------|--------------|---------------|
| 5 Sekunden | ~1-2s | ~2-3s |
| 30 Sekunden | ~3-5s | ~8-12s |
| 1 Minute | ~6-10s | ~15-25s |

## ğŸ”§ Manuelle Installation

Falls du das Script nicht verwenden mÃ¶chtest:

```bash
cd services/whisper-server

# Virtual Environment erstellen
python3 -m venv .venv
source .venv/bin/activate

# Dependencies installieren
pip install -r requirements.txt

# Server starten
export WHISPER_MODEL=base
uvicorn server:app --host 0.0.0.0 --port 9000 --reload
```

## ğŸ› Troubleshooting

### "Port 9000 already in use"
```bash
# Finde den Prozess
lsof -i :9000

# Beende ihn
kill -9 <PID>
```

### "Module 'faster_whisper' not found"
```bash
source .venv/bin/activate
pip install -r requirements.txt
```

### Transkription ist zu langsam
- Verwende kleineres Model (`tiny` oder `base`)
- PrÃ¼fe ob andere Programme die CPU belasten
- ErwÃ¤ge GPU-Nutzung fÃ¼r grÃ¶ÃŸere Models

### Frontend kann Server nicht erreichen
- PrÃ¼fe ob Server lÃ¤uft: `curl http://localhost:9000/healthz`
- PrÃ¼fe Browser-Console auf CORS-Fehler
- Stelle sicher dass beide (Frontend + Server) laufen

## ğŸ“¦ Docker (optional)

```bash
docker build -t whisper-server .
docker run --rm -p 9000:9000 -e WHISPER_MODEL=base whisper-server
```

## ğŸ‰ Vorteile gegenÃ¼ber n8n

âœ… **Schneller**: Keine Netzwerk-Latenz, lokale Verarbeitung  
âœ… **Privat**: Audio verlÃ¤sst nie deinen Computer  
âœ… **Offline**: Funktioniert ohne Internet  
âœ… **Kostenlos**: Keine API-Kosten  
âœ… **ZuverlÃ¤ssig**: Keine Server-AusfÃ¤lle  

---

**Made with â¤ï¸ using Whisper AI**
